# save-felica

## æ¦‚è¦
read-felica ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã€‚ read-felicaã‚ˆã‚Šé€ä»˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡å¾Œã€ãã‚Œã‚’CSVãªã©ã§èª­ã¿è§£ãã€çµæœã‚’Sequelizeã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ ¼ç´ã™ã‚‹ã€‚
é‡è¤‡ã™ã‚‹ã‚‚ã®ãŒã‚ã£ãŸå ´åˆã¯ duplicate ã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã€‚

The backend server for read-felica. After receiving the transit histories from read-felica, this application converts them to something human-readable, and then stores it to the database. 
The database connection is handled by Sequelize, and when one of the transits was already in the database, the whole transaction fails.

## TODO
<ul>
<li> bulk insert ã«ãªã£ã¦ã„ã‚‹ã®ã§ã€ãã‚Œã‚’è¨‚æ­£ã™ã‚‹ </li>
<li>it uses a bulk insert, so need to make it atomic.</li>
<li> <code>node_modules</code> ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã®ã§è«¸ã€…è¨‚æ­£ã™ã‚‹ </li>
     <li> <code>node_modules</code>, which aren't necessary because they are dependencies that can be resolved by doing <code>npm install</code>, so learn how to properly do <code>.gitignore</code></li>
</ul>

## ä½¿ã„æ–¹ How to use
æ—¥æœ¬èªã§ã®èª¬æ˜ã¯å‰²æ„›

First, get yourself a mysql for storage. For example by using <code>docker-compose.yml</code> like this!
Metabase is only there just in case you fancy seeing what you've got via a BI tool.
     
      version: '3.1'

      services:
        mysql:
          image: mysql
          restart: unless-stopped
          ports:
            - 3306:3306
          environment: 
            MYSQL_ROOT_PASSWORD: password
          volumes:
            - mysqlvolume:/var/lib/mysql
        metabase:
          image: metabase/metabase
          restart: unless-stopped
          ports:
            - 3000:3000
          volumes:
            - metabasevolume:/metabase.db  
      volumes:
        mysqlvolume:
        metabasevolume:

Second,

     git clone https://github.com/ihs-ot-as/save-felica
     
     cd save-felica 
     
     nano ./config/config.json 
     #edit the connection as needed
     
     npm install 
     
     sequelize db:create
     sequelize db:migrate 
     #this should apply all the migraion scripts inside ./migrations
     #the database schema is done!
     
     node . -p some_random_string
     #oringally thought of encrypting the communication between the reader and the backend, 
     #so this was meant to be the password for the private key stored in the backend.
     #but it hasn't been implemented so it basically sits there and does nothing.
     

## ã©ã†ã‚„ã£ã¦å‹•ãã® how does it work
å†åº¦ã«ãªã‚‹ãŒã€ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®Qiitaã®æŠ•ç¨¿ãªã—ã§ã¯ã§ããªã‹ã£ãŸã€‚æ·±è¬ã€‚

[https://qiita.com/gebo/items/cb2dd393170767852fb3](https://qiita.com/gebo/items/cb2dd393170767852fb3)

æ›¸ã‹ã‚Œã¦ã„ã‚‹é€šã‚Šã«èª­ã¿æ›¿ãˆã¦ã„ã‚‹ã ã‘ã ãŒã€èª­ã¿æ›¿ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’é€æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿å‡ºã™ã¨é…ããªã‚‹ã®ã§ã€
èµ·å‹•æ™‚ã«ã™ã¹ã¦ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã‚€ã‚ˆã†ã«å·¥å¤«ã—ã¦ã„ã‚‹ã€‚çµæœã€ã‚‚ã—CSVãªã©å¤‰æ›´ãŒã‚ã£ãŸã‚‰å†èµ·å‹•ã—ãªã„ã¨å¤‰æ›´ãŒé©ç”¨ã•ã‚Œãªã„ã“ã¨ã«ãªã‚‹ãŒã€
ãã‚Œã‚ˆã‚Šã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ã»ã†ãŒé‡è¦ã¨ã„ã†åˆ¤æ–­ã§ã‚ã‚‹ã€‚


Once again, this code wouldn't have been possible without the following post on Qiita. Many thanks go to the creator of this postğŸŒ¹

[https://qiita.com/gebo/items/cb2dd393170767852fb3](https://qiita.com/gebo/items/cb2dd393170767852fb3)

This program merely translates the incoming data before saving it to the database, and one detail that I paid attention to is that this program loads all the mappings and stuff into memory upon initialization. This is because reading files on the disk every time is extremely expensive. This means any change to these files requires a restart before they take effect, but in our case performance should be more important.

## ãã®ä»–æ”¹å–„ç‚¹ how could I improve this code?
ã„ã‚ã‚†ã‚‹ enum çš„ãªã¨ã“ã‚ã¯ object ã¤ã¾ã‚Š hashtable ã§ã‚‚ã£ã¦ã„ã‚‹ã®ã ãŒã€CSVã«é–¢ã—ã¦ã¯ãŸã ãã®ã¾ã¾arrayã¨ã—ã¦æŒã£ã¦ã„ã‚‹ã ã‘ã§ã‚ã‚‹ã€‚
ï¼ˆJSã®arrayã¯C#ãªã©ã®ãã®ä»–ã®è¨€èªã«ã‚ã‚‹ã‚ˆã†ã« linked list ã§ã¯ãªãã¦ hashtable ã§ key ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã®ã§ã‚ã‚‹...ãŒã€è«–ç‚¹ã¯åŒã˜ã§æ¤œç´¢ã™ã‚‹ã¨ãã® O(n) ã¯è‰¯ããªã„ï¼‰

ä¸€è¾ºå€’ãªæ¤œç´¢ã®ä»•æ–¹ã—ã‹ã—ãªã„ã®ã§ã€ã“ã‚Œã‚‚ key-value ãŒãŸã«ã—ãŸã»ã†ãŒæ¤œç´¢ãŒæ—©ããªã‚‹ã®ã¯è¨€ã†é–“ã§ã‚‚ãªã„ãŒã€ãƒ‡ãƒ¼ã‚¿ã®åŠ å·¥ã‚’ä¼´ã†ãŸã‚ãã“ã¾ã§è‡³ã£ã¦ã„ãªã„ã€‚



For data that could also be called enums, I store it as pure objects, which in JavaScript are treated as hashtables. However as for the data on stations, it's just an array.
(in JavaScript it's not actually a linked list as seen in languages like C#; rather it's an hashtable where the key is the index... but the problem remains the same, the O(n) isn't very good. )

Since the program only queries for a match always in the same way, I could modify the CSV and make it a pure JS object as well, but I haven't seen such a significant performance issue up until this point.




